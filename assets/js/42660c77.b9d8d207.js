"use strict";(self.webpackChunkcogment_doc=self.webpackChunkcogment_doc||[]).push([[588],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>h});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),m=p(n),h=r,u=m["".concat(s,".").concat(h)]||m[h]||d[h]||o;return n?a.createElement(u,i(i({ref:t},c),{},{components:n})):a.createElement(u,i({ref:t},c))}));function h(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,i[1]=l;for(var p=2;p<o;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},6337:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>l,toc:()=>p});var a=n(3117),r=(n(7294),n(3905));const o={},i="Step 3: Rewards",l={unversionedId:"guide/tutorial/rewards",id:"guide/tutorial/rewards",title:"Step 3: Rewards",description:"This part of the tutorial follows step 2, make sure you've gone through it before starting this one. Alternatively the completed step 2 can be retrieved from the tutorial's repository.",source:"@site/docs/guide/tutorial/3-rewards.md",sourceDirName:"guide/tutorial",slug:"/guide/tutorial/rewards",permalink:"/docs/guide/tutorial/rewards",draft:!1,tags:[],version:"current",lastUpdatedAt:1665763478,formattedLastUpdatedAt:"10/14/2022",sidebarPosition:3,frontMatter:{},sidebar:"docSidebar",previous:{title:"Step 2: Implement a first actor and environment",permalink:"/docs/guide/tutorial/random-player"},next:{title:"Step 4: Add a second actor implementation based on a heuristic",permalink:"/docs/guide/tutorial/heuristic-player"}},s={},p=[{value:"Adding the concept of a game",id:"adding-the-concept-of-a-game",level:2},{value:"Configuring the environment on the client side",id:"configuring-the-environment-on-the-client-side",level:2},{value:"Sending rewards to the actors",id:"sending-rewards-to-the-actors",level:2}],c={toc:p};function d(e){let{components:t,...n}=e;return(0,r.kt)("wrapper",(0,a.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"step-3-rewards"},"Step 3: Rewards"),(0,r.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},"This part of the tutorial follows ",(0,r.kt)("a",{parentName:"p",href:"/docs/guide/tutorial/random-player"},"step 2"),", make sure you've gone through it before starting this one. Alternatively the completed step 2 can be retrieved from the ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/cogment/cogment-tutorial-rps"},"tutorial's repository"),"."))),(0,r.kt)("p",null,"In this step of the tutorial, we will start thinking about rewards. Rewards are a way to evaluate how an actor performs at a task. They can be used to evaluate or compare different implementations of an actor, or, especially in the context of Reinforcement Learning, train a model. In Cogment, both the environment and other actors can evaluate an actor. Here, we will focus on sending rewards from the environment."),(0,r.kt)("p",null,"The first thing we'll do for this step is to add the concept of multi-round games to our RPS implementation. We'll learn to configure the environment along the way. Then, we will adapt the environment implementation to send a reward to the actor winning a game."),(0,r.kt)("h2",{id:"adding-the-concept-of-a-game"},"Adding the concept of a game"),(0,r.kt)("p",null,"Up until now, our implementation of RPS focused on rounds. However, RPS is usually played in games won by the player reaching a target score, i.e. a number of won rounds."),(0,r.kt)("p",null,"Before sending rewards we need to adapt our implementation so that each trial is a game with a configurable target score."),(0,r.kt)("p",null,"The generated data structure ",(0,r.kt)("inlineCode",{parentName:"p"},"EnvironmentConfig"),", referenced within ",(0,r.kt)("inlineCode",{parentName:"p"},"cogment.yaml")," in ",(0,r.kt)("inlineCode",{parentName:"p"},"environment.config_type"),", defines the configuration of the environment. Let's add a ",(0,r.kt)("inlineCode",{parentName:"p"},"target_game_score")," numerical property to it."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-proto"},"message EnvironmentConfig {\n  int32 target_score = 1;\n}\n")),(0,r.kt)("p",null,"Modify the ",(0,r.kt)("inlineCode",{parentName:"p"},"data.proto")," file with this update."),(0,r.kt)("p",null,"The environment implementation can now be updated to know about games."),(0,r.kt)("p",null,"During the ",(0,r.kt)("strong",{parentName:"p"},"initialization")," phase of the ",(0,r.kt)("inlineCode",{parentName:"p"},"environment")," function, we can retrieve the value from the environment's configuration. We also defined a default value in case nothing is specified."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# Default target score\ntarget_score = 3\nif environment_session.config is not None and environment_session.config.target_score >= 0:\n    target_score = environment_session.config.target_score\n")),(0,r.kt)("p",null,"In the ",(0,r.kt)("strong",{parentName:"p"},"event loop")," we need to handle the end of the game, instead of waiting for the client to terminate the trial, the environment will now end it whenever the target score is reached."),(0,r.kt)("p",null,"Once the observation is computed we can decide what to do"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'# Generate and send observations\nobservations = [\n    (p1.actor_name, Observation(me=p1_state, them=p2_state)),\n    (p2.actor_name, Observation(me=p2_state, them=p1_state)),\n]\n\n# Handle end of game\nif state["p1"]["score"] >= target_score:\n    # p1 won\n    environment_session.end(observations)\nelif state["p2"]["score"] >= target_score:\n    # p2 won\n    environment_session.end(observations)\nelse:\n    # target score is not reached\n    environment_session.produce_observations(observations)\n')),(0,r.kt)("p",null,"Modify the ",(0,r.kt)("inlineCode",{parentName:"p"},"environment/main.py")," file with these updates."),(0,r.kt)("p",null,"In this simple implementation, the concept of game is local to the environment. It has no impact on the observation and action spaces, and thus no impact on the actor implementation. This means an actor wouldn't ",(0,r.kt)("em",{parentName:"p"},"know")," that the round it currently plays is the tie breaker in a game or its very first round. As a result the actor will play every round the same way."),(0,r.kt)("h2",{id:"configuring-the-environment-on-the-client-side"},"Configuring the environment on the client side"),(0,r.kt)("p",null,"In the previous steps, we triggered the trials by running ",(0,r.kt)("inlineCode",{parentName:"p"},"./run.sh client_start"),". The more curious among you will have understood that this launches a client of the Cogment app, implemented in ",(0,r.kt)("inlineCode",{parentName:"p"},"client/main.py"),". In this step, we will make changes to this file, this is therefore a good time to take a look at it."),(0,r.kt)("p",null,"In the ",(0,r.kt)("inlineCode",{parentName:"p"},"rps")," directory, the ",(0,r.kt)("inlineCode",{parentName:"p"},"client")," directory contains the python implementation for the cogment client for this app. Take a look a the ",(0,r.kt)("inlineCode",{parentName:"p"},"main.py")," file."),(0,r.kt)("p",null,"After the imports, the first section defines the endpoints of the different services used by the App."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"ORCHESTRATOR_ENDPOINT = f\"grpc://{os.getenv('ORCHESTRATOR_HOST')}:{os.getenv('ORCHESTRATOR_PORT')}\"\nENVIRONMENT_ENDPOINT = f\"grpc://{os.getenv('ENVIRONMENT_HOST')}:{os.getenv('ENVIRONMENT_PORT')}\"\nRANDOM_AGENT_ENDPOINT = f\"grpc://{os.getenv('RANDOM_AGENT_HOST')}:{os.getenv('RANDOM_AGENT_PORT')}\"\n")),(0,r.kt)("p",null,"Notice the use of the environment variables defined in the ",(0,r.kt)("inlineCode",{parentName:"p"},".env")," file."),(0,r.kt)("p",null,"The other part of the file is the ",(0,r.kt)("inlineCode",{parentName:"p"},"main")," function."),(0,r.kt)("p",null,"The first section creates a controller instance to be able to handle the lifecycle of the trials for the App's orchestrator's instance."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'context = cogment.Context(cog_settings=cog_settings, user_id="rps")\n\n# Create a controller\ncontroller = context.get_controller(endpoint=cogment.Endpoint(ORCHESTRATOR_ENDPOINT))\n')),(0,r.kt)("p",null,"The bulk of the main function is defining the parameters for the trial: how many actors will be involved, their respective classes, which implementation and endpoint to use and their configuration, it also defines the same information for the environment."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'# Define parameters for 2 actors using the same `random_agent` implementation\nactor_1_params = cogment.ActorParameters(\n    cog_settings,\n    name="player_1",\n    class_name="player",\n    endpoint=RANDOM_AGENT_ENDPOINT,\n    implementation="random_agent"\n)\nactor_2_params = cogment.ActorParameters(\n    cog_settings,\n    name="player_2",\n    class_name="player",\n    endpoint=RANDOM_AGENT_ENDPOINT,\n    implementation="random_agent"\n)\n\n# Assemble everything in the trial parameters\ntrial_params=cogment.TrialParameters(\n    cog_settings,\n    environment_name="env",\n    environment_endpoint=ENVIRONMENT_ENDPOINT,\n    environment_config=EnvironmentConfig(),\n    actors=[\n        actor_1_params,\n        actor_2_params,\n    ]\n)\n')),(0,r.kt)("p",null,"To learn more about the parameters, check the ",(0,r.kt)("a",{parentName:"p",href:"/docs/reference/python#class-cogmenttrialparameters"},(0,r.kt)("inlineCode",{parentName:"a"},"cogment.TrialParameters"))," class reference."),(0,r.kt)("p",null,"Finally the last section starts the trial, waits 10 seconds and then terminates it."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# Start a new trial using the trial params we just created\ntrial_id = await controller.start_trial(trial_params=trial_params)\nprint(f\"Trial '{trial_id}' started\")\n\n# Let the trial play for a while\nawait asyncio.sleep(10)\n\n# Termination the trial\nawait controller.terminate_trial([trial_id])\nprint(f\"Trial '{trial_id}' terminated\")\n")),(0,r.kt)("p",null,"Going back to the task at hand, now that the ",(0,r.kt)("inlineCode",{parentName:"p"},"EnvironmentConfig")," is modified and the environment implementation uses it, we need to update the client to:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"configure the target score."),(0,r.kt)("li",{parentName:"ol"},"wait for the environment to end the trial (when the target score is reached), instead of letting the trial run for a few seconds.")),(0,r.kt)("p",null,"The first change we need to make is configuring the target score."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'# Configure the environment\nenvironment_config=EnvironmentConfig(\n    target_score=5\n)\n\n# Assemble everything in the trial parameters\ntrial_params=cogment.TrialParameters(\n    cog_settings,\n    environment_name="env",\n    environment_endpoint=ENVIRONMENT_ENDPOINT,\n    environment_config=environment_config,\n    actors=[\n        actor_1_params,\n        actor_2_params,\n    ]\n)\n')),(0,r.kt)("p",null,"We then need to slightly adapt how we handle the trial lifecycle to support the trial termination coming from the environment. We will:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Have the client define the identifier of the trial to be able to listen for its termination,"),(0,r.kt)("li",{parentName:"ul"},"Start the trial using the requested ",(0,r.kt)("inlineCode",{parentName:"li"},"trial_id")),(0,r.kt)("li",{parentName:"ul"},"Use ",(0,r.kt)("inlineCode",{parentName:"li"},"controller.watch_trials")," to wait for the trial to finish.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'# Defining the trial id on the client side (Don\'t forget to add `import datetime` at the top of the file)\ntrial_id=f"rps-{datetime.datetime.now().isoformat()}"\n\n# Listening for ended trials\nasync def await_trial():\n    async for trial_info in controller.watch_trials(trial_state_filters=[cogment.TrialState.ENDED]):\n        if trial_info.trial_id == trial_id:\n            break\nawait_trial_task = asyncio.create_task(await_trial())\n\n# Start a new trial using the trial params we just created\ntrial_id = await controller.start_trial(trial_id_requested=trial_id, trial_params=trial_params)\nprint(f"Trial \'{trial_id}\' started")\n\n# Wait for the trial to end\nawait await_trial_task\nprint(f"Trial \'{trial_id}\' ended")\n')),(0,r.kt)("div",{className:"admonition admonition-tip alert alert--success"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"tip")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},"Because of the asynchronous nature of what's happening here, we need to start listening for the trial to end before starting it."))),(0,r.kt)("p",null,"You can now ",(0,r.kt)("a",{parentName:"p",href:"/docs/guide/tutorial/bootstrap-and-data-structures#building-and-running-the-app"},"build and run")," the application to check that it works as expected."),(0,r.kt)("h2",{id:"sending-rewards-to-the-actors"},"Sending rewards to the actors"),(0,r.kt)("p",null,"The environment is now able to:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"compute when an actor wins a game,"),(0,r.kt)("li",{parentName:"ul"},"communicate this information to it and to the other Cogment app services,"),(0,r.kt)("li",{parentName:"ul"},"send ",(0,r.kt)("strong",{parentName:"li"},"rewards")," when an actor reaches a measurable goal, in our case, when it wins a game.")),(0,r.kt)("p",null,"Please note, that not all actions need to be rewarded."),(0,r.kt)("p",null,"When a game is won, the environment will add a ",(0,r.kt)("strong",{parentName:"p"},"positive reward to the winner")," (we chose a value of 1) and a ",(0,r.kt)("strong",{parentName:"p"},"negative reward to the loser")," (we chose a value of -1). Cogment also supports the notion of ",(0,r.kt)("em",{parentName:"p"},"confidence"),", a weight between 0 and 1 that expresses the qualification of the reward sender in its appreciation. In this case we are applying objective rules, so we use a confidence of 1."),(0,r.kt)("p",null,"In the ",(0,r.kt)("strong",{parentName:"p"},"event loop"),", when the first player wins a game we add the following."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"environment_session.add_reward(value=1, confidence=1, to=[p1.actor_name])\nenvironment_session.add_reward(value=-1, confidence=1, to=[p2.actor_name])\n")),(0,r.kt)("p",null,"When the second player wins a game we add the following."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"environment_session.add_reward(value=-1, confidence=1, to=[p1.actor_name])\nenvironment_session.add_reward(value=1, confidence=1, to=[p2.actor_name])\n")),(0,r.kt)("p",null,"Modify the ",(0,r.kt)("inlineCode",{parentName:"p"},"environment/main.py")," file to include the above additions."),(0,r.kt)("p",null,"You can now ",(0,r.kt)("a",{parentName:"p",href:"/docs/guide/tutorial/bootstrap-and-data-structures#building-and-running-the-app"},"build and run")," the application to check that it works as expected. In particular you should see logs relative to the reception of rewards on the actor side."),(0,r.kt)("p",null,"This concludes the step 3 of the tutorial: you've learned about environment configuration, about how to let the environment control the termination of the trial and you implemented reward sending."),(0,r.kt)("p",null,"Let\u2019s move on to implementing an RPS player that actually considers what was played before deciding on its next move in ",(0,r.kt)("a",{parentName:"p",href:"/docs/guide/tutorial/heuristic-player"},"step 4"),"."))}d.isMDXComponent=!0}}]);