"use strict";(self.webpackChunkcogment_doc=self.webpackChunkcogment_doc||[]).push([[278],{3905:function(e,t,n){n.d(t,{Zo:function(){return c},kt:function(){return u}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),m=p(n),u=r,h=m["".concat(s,".").concat(u)]||m[u]||d[u]||i;return n?a.createElement(h,o(o({ref:t},c),{},{components:n})):a.createElement(h,o({ref:t},c))}));function u(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,o[1]=l;for(var p=2;p<i;p++)o[p]=n[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},4632:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return l},contentTitle:function(){return s},metadata:function(){return p},toc:function(){return c},default:function(){return m}});var a=n(7462),r=n(3366),i=(n(7294),n(3905)),o=["components"],l={},s="Step 4: Add a second actor implementation based on a heuristic",p={unversionedId:"guide/tutorial/heuristic-player",id:"guide/tutorial/heuristic-player",title:"Step 4: Add a second actor implementation based on a heuristic",description:"This part of the tutorial follows step 3, make sure you've gone through it before starting this one. Alternatively the completed step 3 can be retrieved from the tutorial's repository.",source:"@site/docs/guide/tutorial/4-heuristic-player.md",sourceDirName:"guide/tutorial",slug:"/guide/tutorial/heuristic-player",permalink:"/docs/guide/tutorial/heuristic-player",tags:[],version:"current",lastUpdatedAt:1649768289,formattedLastUpdatedAt:"4/12/2022",sidebarPosition:4,frontMatter:{},sidebar:"docSidebar",previous:{title:"Step 3: Rewards",permalink:"/docs/guide/tutorial/rewards"},next:{title:"Step 5: Add a human player in the loop",permalink:"/docs/guide/tutorial/human-player"}},c=[{value:"Creating a second actor implementation",id:"creating-a-second-actor-implementation",children:[],level:2},{value:"Implementing a simple heuristic&#39;s agent",id:"implementing-a-simple-heuristics-agent",children:[],level:2}],d={toc:c};function m(e){var t=e.components,n=(0,r.Z)(e,o);return(0,i.kt)("wrapper",(0,a.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"step-4-add-a-second-actor-implementation-based-on-a-heuristic"},"Step 4: Add a second actor implementation based on a heuristic"),(0,i.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"This part of the tutorial follows ",(0,i.kt)("a",{parentName:"p",href:"/docs/guide/tutorial/rewards"},"step 3"),", make sure you've gone through it before starting this one. Alternatively the completed step 3 can be retrieved from the ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/cogment/cogment-tutorial-rps"},"tutorial's repository"),"."))),(0,i.kt)("p",null,"In this step of the tutorial, we will go over another actor implementation and learn about using the received observations before doing an action."),(0,i.kt)("h2",{id:"creating-a-second-actor-implementation"},"Creating a second actor implementation"),(0,i.kt)("p",null,"Let's start by creating another implementation of the ",(0,i.kt)("inlineCode",{parentName:"p"},"player")," actor class. Because we expect it to be rather small and not use additional dependencies, this second implementation will ",(0,i.kt)("em",{parentName:"p"},"live")," in the same service as the previous one. We will start by copying the ",(0,i.kt)("inlineCode",{parentName:"p"},"random_agent")," implementation."),(0,i.kt)("p",null,"In ",(0,i.kt)("inlineCode",{parentName:"p"},"random_agent/main.py")," copy/paste the ",(0,i.kt)("inlineCode",{parentName:"p"},"random_agent")," function and name it ",(0,i.kt)("inlineCode",{parentName:"p"},"heuristic_agent"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"async def heuristic_agent(actor_session):\n")),(0,i.kt)("p",null,'Then, in the same file, register this "new" implementation in the ',(0,i.kt)("inlineCode",{parentName:"p"},"main")," function."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'context.register_actor(\n  impl=heuristic_agent,\n  impl_name="heuristic_agent",\n  actor_classes=["player"])\n')),(0,i.kt)("p",null,"When the service starts it will now host the two implementations."),(0,i.kt)("p",null,"We can now configure one of the ",(0,i.kt)("inlineCode",{parentName:"p"},"player")," in the default trial, defined in ",(0,i.kt)("inlineCode",{parentName:"p"},"client/main.py"),", to use the ",(0,i.kt)("inlineCode",{parentName:"p"},"heuristic_agent")," implementation."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'actor_2_params = cogment.ActorParameters(\n    cog_settings,\n    name="player_2",\n    class_name="player",\n    endpoint=RANDOM_AGENT_ENDPOINT,\n    implementation="heuristic_agent"\n)\n')),(0,i.kt)("p",null,"Modify the ",(0,i.kt)("inlineCode",{parentName:"p"},"client/main.py")," file to include the above addition."),(0,i.kt)("p",null,"You can now ",(0,i.kt)("a",{parentName:"p",href:"/docs/guide/tutorial/bootstrap-and-data-structures#building-and-running-the-app"},"build and run")," the application to check that it still works. Nothing should have changed except one of the player uses the code from the new implementation."),(0,i.kt)("h2",{id:"implementing-a-simple-heuristics-agent"},"Implementing a simple heuristic's agent"),(0,i.kt)("p",null,"While the ",(0,i.kt)("inlineCode",{parentName:"p"},"random_player")," ignored the state of the game, picking its move at random, our new implementation will consider the received ",(0,i.kt)("strong",{parentName:"p"},"observations")," to pick its move."),(0,i.kt)("p",null,"We will implement a subset of the strategies described in ",(0,i.kt)("a",{parentName:"p",href:"https://towardsai.net/p/artificial-intelligence/towards-an-ai-for-rock-paper-scissors-3fb05780271f"},"this")," article:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"If I won the last round, do the same thing,"),(0,i.kt)("li",{parentName:"ul"},"If my opponent won the last round, play the move that would have won against his,"),(0,i.kt)("li",{parentName:"ul"},"If the last round was a draw, play a random move.")),(0,i.kt)("p",null,"We will start by redefining in ",(0,i.kt)("inlineCode",{parentName:"p"},"random_agent/main.py")," the same ",(0,i.kt)("inlineCode",{parentName:"p"},"DEFEATS")," we used by the environment."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"DEFEATS = {\n  ROCK: PAPER,\n  SCISSORS: ROCK,\n  PAPER: SCISSORS\n}\n")),(0,i.kt)("p",null,"Then, in the event loop, we look at the received observation before taking an action based on this simple strategy."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"observation = event.observation\nprint(f\"'{actor_session.name}' received an observation: '{observation}'\")\nif event.type == cogment.EventType.ACTIVE:\n    if observation.snapshot.me.won_last:\n        # I won the last round, let's play the same thing\n        actor_session.do_action(PlayerAction(move=observation.snapshot.me.last_move))\n    elif observation.snapshot.them.won_last:\n        # I lost the last round, let's play what would have won\n        actor_session.do_action(PlayerAction(move=DEFEATS[observation.snapshot.them.last_move]))\n    else:\n        # last round was a draw, let's play randomly\n        actor_session.do_action(PlayerAction(move=random.choice(MOVES)))\n")),(0,i.kt)("p",null,"Modify the ",(0,i.kt)("inlineCode",{parentName:"p"},"random_player/main.py")," file accordingly."),(0,i.kt)("p",null,"You can now ",(0,i.kt)("a",{parentName:"p",href:"/docs/guide/tutorial/bootstrap-and-data-structures#building-and-running-the-app"},"build and run")," the application to check that it works. Don't expect the heuristic player to beat the random player, the nature of the game actually rewards pure randomness in the playing. You can however implement various strategies and see how they fare against each other."),(0,i.kt)("p",null,"This concludes the step 4 of the tutorial: you've learned about adding and using different implementations of an actor class and how to access and use the received observations."),(0,i.kt)("p",null,"Let\u2019s move on to adding a human player in the mix with ",(0,i.kt)("a",{parentName:"p",href:"/docs/guide/tutorial/human-player"},"step 5"),"."))}m.isMDXComponent=!0}}]);